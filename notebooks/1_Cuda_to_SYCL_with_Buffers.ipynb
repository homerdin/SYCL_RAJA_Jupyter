{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Point\n",
    "### Existing CUDA code\n",
    "\n",
    "```c\n",
    "namespace rajaperf\n",
    "{\n",
    "namespace basic\n",
    "{\n",
    "\n",
    "  //\n",
    "  // Define thread block size for CUDA execution\n",
    "  //\n",
    "  const size_t block_size = 256;\n",
    "\n",
    "\n",
    "#define DAXPY_DATA_SETUP_CUDA \\\n",
    "  allocAndInitCudaDeviceData(x, m_x, iend); \\\n",
    "  allocAndInitCudaDeviceData(y, m_y, iend);\n",
    "\n",
    "#define DAXPY_DATA_TEARDOWN_CUDA \\\n",
    "  getCudaDeviceData(m_y, y, iend); \\\n",
    "  deallocCudaDeviceData(x); \\\n",
    "  deallocCudaDeviceData(y);\n",
    "\n",
    "__global__ void daxpy(Real_ptr y, Real_ptr x,\n",
    "                      Real_type a,\n",
    "                      Index_type iend)\n",
    "{\n",
    "   Index_type i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "   if (i < iend) {\n",
    "     DAXPY_BODY;\n",
    "   }\n",
    "}\n",
    "\n",
    "\n",
    "void DAXPY::runCudaVariant(VariantID vid)\n",
    "{\n",
    "  const Index_type run_reps = getRunReps();\n",
    "  const Index_type ibegin = 0;\n",
    "  const Index_type iend = getRunSize();\n",
    "\n",
    "  DAXPY_DATA_SETUP;\n",
    "\n",
    "  if ( vid == Base_CUDA ) {\n",
    "\n",
    "    DAXPY_DATA_SETUP_CUDA;\n",
    "\n",
    "    startTimer();\n",
    "    for (RepIndex_type irep = 0; irep < run_reps; ++irep) {\n",
    "\n",
    "      const size_t grid_size = RAJA_DIVIDE_CEILING_INT(iend, block_size);\n",
    "      daxpy<<<grid_size, block_size>>>( y, x, a,\n",
    "                                        iend );\n",
    "\n",
    "    }\n",
    "    stopTimer();\n",
    "\n",
    "    DAXPY_DATA_TEARDOWN_CUDA;\n",
    "\n",
    "  } else if ( vid == RAJA_CUDA ) {\n",
    "\n",
    "    DAXPY_DATA_SETUP_CUDA;\n",
    "\n",
    "    startTimer();\n",
    "    for (RepIndex_type irep = 0; irep < run_reps; ++irep) {\n",
    "\n",
    "      RAJA::forall< RAJA::cuda_exec<block_size, true /*async*/> >(\n",
    "        RAJA::RangeSegment(ibegin, iend), [=] __device__ (Index_type i) {\n",
    "        DAXPY_BODY;\n",
    "      });\n",
    "\n",
    "    }\n",
    "    stopTimer();\n",
    "\n",
    "    DAXPY_DATA_TEARDOWN_CUDA;\n",
    "\n",
    "  } else {\n",
    "     std::cout << \"\\n  DAXPY : Unknown Cuda variant id = \" << vid << std::endl;\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "## Kernel \n",
    "\n",
    "## Kernel Launch\n",
    "\n",
    "## Data Teardown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"DAXPY.hpp\"\n",
    "\n",
    "#include \"RAJA/RAJA.hpp\"\n",
    "\n",
    "#if defined(RAJA_ENABLE_SYCL)\n",
    "\n",
    "#include \"common/SyclDataUtils.hpp\"\n",
    "\n",
    "#include <iostream>\n",
    "\n",
    "namespace rajaperf\n",
    "{\n",
    "namespace basic\n",
    "{\n",
    "\n",
    "  //\n",
    "  // Define thread block size for SYCL execution\n",
    "  //\n",
    "  const size_t block_size = 256;\n",
    "\n",
    "#define DAXPY_DATA_SETUP_SYCL \\\n",
    "  allocAndInitSyclDeviceData(x, m_x, iend, qu); \\\n",
    "  allocAndInitSyclDeviceData(y, m_y, iend, qu);\n",
    "\n",
    "#define DAXPY_DATA_TEARDOWN_SYCL \\\n",
    "  getSyclDeviceData(m_y, y, iend, qu); \\\n",
    "  deallocSyclDeviceData(x, qu); \\\n",
    "  deallocSyclDeviceData(y, qu);\n",
    "\n",
    "\n",
    "void DAXPY::runSyclVariant(VariantID vid)\n",
    "{\n",
    "  const Index_type run_reps = getRunReps();\n",
    "  const Index_type ibegin = 0;\n",
    "  const Index_type iend = getRunSize();\n",
    "\n",
    "  DAXPY_DATA_SETUP;\n",
    "\n",
    "  if ( vid == Base_SYCL ) {\n",
    "\n",
    "    DAXPY_DATA_SETUP_SYCL;\n",
    "\n",
    "    startTimer();\n",
    "    for (RepIndex_type irep = 0; irep < run_reps; ++irep) {\n",
    "\n",
    "      const size_t grid_size = block_size * RAJA_DIVIDE_CEILING_INT(iend, block_size);\n",
    "\n",
    "      qu.submit([&] (cl::sycl::handler& h) {\n",
    "        h.parallel_for<class DAXPY>(cl::sycl::nd_range<1>{grid_size, block_size},\n",
    "                                    [=] (cl::sycl::nd_item<1> item ) {\n",
    "\n",
    "          Index_type i = item.get_global_id(0);\n",
    "          if (i < iend) {\n",
    "            DAXPY_BODY\n",
    "          }\n",
    "\n",
    "        });\n",
    "      });\n",
    "    }\n",
    "    qu.wait(); // Wait for computation to finish before stopping timer\n",
    "    stopTimer();\n",
    "\n",
    "    DAXPY_DATA_TEARDOWN_SYCL;\n",
    "\n",
    "  } else if ( vid == RAJA_SYCL ) {\n",
    "\n",
    "    DAXPY_DATA_SETUP_SYCL;\n",
    "    startTimer();\n",
    "    for (RepIndex_type irep = 0; irep < run_reps; ++irep) {\n",
    "\n",
    "      RAJA::forall< RAJA::sycl_exec<block_size, true> >(\n",
    "        RAJA::RangeSegment(ibegin, iend), [=] (Index_type i) {\n",
    "        DAXPY_BODY;\n",
    "      });\n",
    "\n",
    "    }\n",
    "    qu.wait();\n",
    "    stopTimer();\n",
    "\n",
    "    DAXPY_DATA_TEARDOWN_SYCL;\n",
    "\n",
    "  } else {\n",
    "     std::cout << \"\\n  DAXPY : Unknown Sycl variant id = \" << vid << std::endl;\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "} // end namespace basic\n",
    "} // end namespace rajaperf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
