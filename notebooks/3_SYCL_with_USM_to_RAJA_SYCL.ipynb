{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYCL with USM to RAJA-SYCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Point\n",
    "Now that we have an `nd_range` with USM implementation in SYCL, we can write the corresponding code in RAJA. The RAJA-SYCL backend that we will use is a work in progress and not yet upstream in RAJA.  The working draft can be found at https://github.com/homerdin/RAJA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "#include \"DAXPY.hpp\"\n",
    "\n",
    "#include \"RAJA/RAJA.hpp\"\n",
    "\n",
    "#if defined(RAJA_ENABLE_SYCL)\n",
    "\n",
    "#include \"common/SyclDataUtils.hpp\"\n",
    "\n",
    "#include <iostream>\n",
    "\n",
    "namespace rajaperf\n",
    "{\n",
    "namespace basic\n",
    "{\n",
    "\n",
    "  //\n",
    "  // Define thread block size for SYCL execution\n",
    "  //\n",
    "  const size_t block_size = 256;\n",
    "\n",
    "#define DAXPY_DATA_SETUP_SYCL \\\n",
    "  allocAndInitSyclDeviceData(x, m_x, iend, qu); \\\n",
    "  allocAndInitSyclDeviceData(y, m_y, iend, qu);\n",
    "\n",
    "#define DAXPY_DATA_TEARDOWN_SYCL \\\n",
    "  getSyclDeviceData(m_y, y, iend, qu); \\\n",
    "  deallocSyclDeviceData(x, qu); \\\n",
    "  deallocSyclDeviceData(y, qu);\n",
    "\n",
    "\n",
    "void DAXPY::runSyclVariant(VariantID vid)\n",
    "{\n",
    "  const Index_type run_reps = getRunReps();\n",
    "  const Index_type ibegin = 0;\n",
    "  const Index_type iend = getRunSize();\n",
    "\n",
    "  DAXPY_DATA_SETUP;\n",
    "\n",
    "  if ( vid == Base_SYCL ) {\n",
    "\n",
    "    DAXPY_DATA_SETUP_SYCL;\n",
    "\n",
    "    startTimer();\n",
    "    for (RepIndex_type irep = 0; irep < run_reps; ++irep) {\n",
    "\n",
    "      const size_t global_size = block_size * RAJA_DIVIDE_CEILING_INT(iend, block_size);\n",
    "\n",
    "      qu.submit([&] (cl::sycl::handler& h) {\n",
    "        h.parallel_for<class DAXPY>(cl::sycl::nd_range<1>{global_size, block_size},\n",
    "                                    [=] (cl::sycl::nd_item<1> item ) {\n",
    "\n",
    "          Index_type i = item.get_global_id(0);\n",
    "          if (i < iend) {\n",
    "            DAXPY_BODY\n",
    "          }\n",
    "\n",
    "        });\n",
    "      });\n",
    "    }\n",
    "    qu.wait(); // Wait for computation to finish before stopping timer\n",
    "    stopTimer();\n",
    "\n",
    "    DAXPY_DATA_TEARDOWN_SYCL;\n",
    "\n",
    "  } else if ( vid == RAJA_SYCL ) {\n",
    "\n",
    "      // Lets fill this in\n",
    "      \n",
    "  } else {\n",
    "     std::cout << \"\\n  DAXPY : Unknown Sycl variant id = \" << vid << std::endl;\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "} // end namespace basic\n",
    "} // end namespace rajaperf\n",
    "\n",
    "#endif  // RAJA_ENABLE_SYCL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "We will continue to use the USM implementations for the memory management, so no changes are needed.  \n",
    "\n",
    "RAJA is an abstraction for portable, parallel loop execution.  RAJA does not provide any abstractions over memory management.  There is a related project UMPIRE which abstracts away programming model specific memory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Launch\n",
    "The RAJA kernel launch look similar to the sycl kernel launch.  However we do not need to calculate the global size and guard the execution of the body, this is handled by RAJA.  Instead we create a `RAJA::forall` loop, which will use the `sycl_exec` execution policy.  This tells RAJA to use the SYCL backend.  The template parameters, `<block_size, true>`, specify the block size to use and whether to launch the loop asynchronously.  \n",
    "```c\n",
    "      RAJA::forall< RAJA::sycl_exec<block_size, true> >(\n",
    "        RAJA::RangeSegment(ibegin, iend), [=] (Index_type i) {\n",
    "        DAXPY_BODY;\n",
    "      });\n",
    "```\n",
    "\n",
    "When coming from an existing RAJA implementation all that is needed is to changes the execution policy to the one you are now targeting (OpenMPTarget or CUDA below).  So everything would be the same except we need to change :\n",
    "```c\n",
    "RAJA::sycl_exec<block_size, true>\n",
    "```\n",
    "to \n",
    "```c\n",
    "RAJA::omp_target_parallel_for_exec<threads_per_team>\n",
    "``` \n",
    "or \n",
    "```c\n",
    "RAJA::cuda_exec<block_size, true>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel\n",
    "RAJA maps our iteration location to the given index.  This is straightforward for a basic `forall`, but with more complex kernel structures we can easily change how to map our index variables to the kernel iteration locations.\n",
    "```c\n",
    "      [=] (Index_type i) {\n",
    "        DAXPY_BODY;\n",
    "      });\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Teardown\n",
    "Again we will use the existing USM implementation for memory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~//\n",
    "// Copyright (c) 2017-19, Lawrence Livermore National Security, LLC.\n",
    "//\n",
    "// Produced at the Lawrence Livermore National Laboratory\n",
    "//\n",
    "// LLNL-CODE-738930\n",
    "//\n",
    "// All rights reserved.\n",
    "//\n",
    "// This file is part of the RAJA Performance Suite.\n",
    "//\n",
    "// For details about use and distribution, please read RAJAPerf/LICENSE.\n",
    "//\n",
    "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~//\n",
    "\n",
    "#include \"DAXPY.hpp\"\n",
    "\n",
    "#include \"RAJA/RAJA.hpp\"\n",
    "\n",
    "#if defined(RAJA_ENABLE_SYCL)\n",
    "\n",
    "#include \"common/SyclDataUtils.hpp\"\n",
    "\n",
    "#include <iostream>\n",
    "\n",
    "namespace rajaperf\n",
    "{\n",
    "namespace basic\n",
    "{\n",
    "\n",
    "  //\n",
    "  // Define thread block size for SYCL execution\n",
    "  //\n",
    "  const size_t block_size = 256;\n",
    "\n",
    "#define DAXPY_DATA_SETUP_SYCL \\\n",
    "  allocAndInitSyclDeviceData(x, m_x, iend, qu); \\\n",
    "  allocAndInitSyclDeviceData(y, m_y, iend, qu);\n",
    "\n",
    "#define DAXPY_DATA_TEARDOWN_SYCL \\\n",
    "  getSyclDeviceData(m_y, y, iend, qu); \\\n",
    "  deallocSyclDeviceData(x, qu); \\\n",
    "  deallocSyclDeviceData(y, qu);\n",
    "\n",
    "\n",
    "void DAXPY::runSyclVariant(VariantID vid)\n",
    "{\n",
    "  const Index_type run_reps = getRunReps();\n",
    "  const Index_type ibegin = 0;\n",
    "  const Index_type iend = getRunSize();\n",
    "\n",
    "  DAXPY_DATA_SETUP;\n",
    "\n",
    "  if ( vid == Base_SYCL ) {\n",
    "\n",
    "    // We already did this\n",
    "\n",
    "  } else if ( vid == RAJA_SYCL ) {\n",
    "\n",
    "    DAXPY_DATA_SETUP_SYCL;\n",
    "    startTimer();\n",
    "    for (RepIndex_type irep = 0; irep < run_reps; ++irep) {\n",
    "\n",
    "      RAJA::forall< RAJA::sycl_exec<block_size, true> >(\n",
    "        RAJA::RangeSegment(ibegin, iend), [=] (Index_type i) {\n",
    "        DAXPY_BODY;\n",
    "      });\n",
    "\n",
    "    }\n",
    "    qu.wait();\n",
    "    stopTimer();\n",
    "\n",
    "    DAXPY_DATA_TEARDOWN_SYCL;\n",
    "\n",
    "  } else {\n",
    "     std::cout << \"\\n  DAXPY : Unknown Sycl variant id = \" << vid << std::endl;\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "} // end namespace basic\n",
    "} // end namespace rajaperf\n",
    "\n",
    "#endif  // RAJA_ENABLE_SYCL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Lets Run It;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
