{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAJA-SYCL work in progress backend\n",
    "\n",
    "RAJA kernel execution is determined by the policies in the template argument.  We will see how these execution policies are mapped to execution of a SYCL kernel in the internels of the RAJA-SYCL work in progress backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAJA-SYCL forall\n",
    "The `RAJA::forall` execution takes an execution policy, `<exec_policy>`, as a template parameter.  To enable `RAJA::forall` with the SYCL programming model, we implement the `sycl_exec<block_size, async>` execution policy.  We also have a temporary `sycl_exec_trivial<block_size, async>` execution policy for kernels which are trivially copyable.  \n",
    "\n",
    "The `sycl_exec_trivial` policy implementation is show below.  It determines a `globalSize` which is a multiple of the specified `blockSize`.  It then launches a SYCL kernel using an `nd_range` and executes the kernel lambda with the RAJA index variable.\n",
    "```c\n",
    "template <typename Iterable, typename LoopBody, size_t BlockSize, bool Async>\n",
    "RAJA_INLINE void forall_impl(sycl_exec_trivial<BlockSize, Async>,\n",
    "                             Iterable&& iter,\n",
    "                             LoopBody&& loop_body)\n",
    "{\n",
    "\n",
    "  using Iterator  = camp::decay<decltype(std::begin(iter))>;\n",
    "  using LOOP_BODY = camp::decay<LoopBody>;\n",
    "  using IndexType = camp::decay<decltype(std::distance(std::begin(iter), std::end(iter)))>;\n",
    "\n",
    "  //\n",
    "  // Compute the requested iteration space size\n",
    "  //\n",
    "  Iterator begin = std::begin(iter);\n",
    "  Iterator end = std::end(iter);\n",
    "  IndexType len = std::distance(begin, end);\n",
    "\n",
    "  // Only launch kernel if we have something to iterate over\n",
    "  if (len > 0 && BlockSize > 0) {\n",
    "\n",
    "    //\n",
    "    // Compute the number of blocks\n",
    "    //\n",
    "    sycl_dim_t blockSize{BlockSize};\n",
    "    sycl_dim_t globalSize = impl::getGridDim(static_cast<size_t>(len), BlockSize);\n",
    "\n",
    "    cl::sycl::queue* q = ::RAJA::sycl::detail::getQueue();\n",
    "\n",
    "    q->submit([&](cl::sycl::handler& h) {\n",
    "\n",
    "      h.parallel_for( cl::sycl::nd_range<1>{globalSize, blockSize},\n",
    "                      [=]  (cl::sycl::nd_item<1> it) {\n",
    "\n",
    "        size_t ii = it.get_global_id(0);\n",
    "\n",
    "        if (ii < len) {\n",
    "          loop_body(begin[ii]);\n",
    "        }\n",
    "      });\n",
    "    });\n",
    "\n",
    "    if (!Async) { q->wait(); }\n",
    "\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAJA-SYCL Kernel\n",
    "The `RAJA::kernel` execution takes a number of `RAJA::statement`s to specify how to execute the kernel.  For the RAJA-SYCL work in progress backend we implement the `SyclKernel` and `SyclKernelTrivial` statements to launch a SYCL kernel.  Then we use the SYCL kernel policies, eg. `RAJA::sycl_global_1<256>`, to be used by  `RAJA::statement::For` to specify how ranges and indexes are mapped to from the RAJA kernel to the SYCL kernel.\n",
    "\n",
    "Shown below is the SYCL kernel launch specified with the `SyclKernelTrivial` statement. \n",
    "\n",
    "\n",
    "```c\n",
    "template<bool async0, typename StmtList, typename Data, typename Types>\n",
    "struct SyclLaunchHelperTrivial<sycl_launch<async0>,StmtList,Data,Types>\n",
    "{\n",
    "  using Self = SyclLaunchHelperTrivial;\n",
    "\n",
    "  static constexpr bool async = async0;\n",
    "\n",
    "  using executor_t = internal::sycl_statement_list_executor_t<StmtList, Data, Types>;\n",
    "  using data_t = camp::decay<Data>;\n",
    "\n",
    "  static void launch(Data &&data,\n",
    "                     internal::LaunchDims launch_dims,\n",
    "                     size_t shmem,\n",
    "                     cl::sycl::queue* qu)\n",
    "  {\n",
    "\n",
    "    qu->submit([&](cl::sycl::handler& h) {\n",
    "\n",
    "      h.parallel_for(launch_dims.fit_nd_range(),\n",
    "                     [=] (cl::sycl::nd_item<3> item) {\n",
    "\n",
    "        SyclKernelLauncher<Data, executor_t>(data, item);\n",
    "\n",
    "      });\n",
    "    });\n",
    "\n",
    "    if (!async) { stream->wait(); };\n",
    "\n",
    "  }\n",
    "};\n",
    "\n",
    "```\n",
    "\n",
    "The inner statement, `sycl_global_1<256>` is called through it's exec function, shown below.  The SYCL kernel id is mapped to the RAJA kernel index and the next enclosed `RAJA::statement` is `exec`.\n",
    "\n",
    "```c\n",
    "  static\n",
    "  inline RAJA_DEVICE void exec(Data &data, cl::sycl::nd_item<3> item, bool thread_active)\n",
    "  {\n",
    "    auto len = segment_length<ArgumentId>(data);\n",
    "    auto i = item.get_global_id(Dim);\n",
    "\n",
    "      // Assign the x thread to the argument\n",
    "      data.template assign_offset<ArgumentId>(i);\n",
    "\n",
    "      // execute enclosed statements\n",
    "      enclosed_stmts_t::exec(data, item, thread_active && (i<len));\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAJA-SYCL reduction object\n",
    "Part of the current work in progress version of the RAJA reduction objects with the `sycl_reduce` policy is shown below.  Currently the SYCL reduction object only works with the 1D `forall` execution.\n",
    "\n",
    "This reduces using `atomic_ref` to an array of work group size, by accessing the local id through the `__spriv` extension.\n",
    "\n",
    "```c\n",
    "//! specialization of ReduceSum for SYCL\n",
    "template <typename T>\n",
    "class ReduceSum<sycl_reduce, T>\n",
    "    : public TargetReduce<RAJA::reduce::sum<T>, T>\n",
    "{\n",
    "public:\n",
    "\n",
    "  using self = ReduceSum<sycl_reduce, T>;\n",
    "  using parent = TargetReduce<RAJA::reduce::sum<T>, T>;\n",
    "  using parent::parent;\n",
    "\n",
    "  //! enable operator+= for ReduceSum -- alias for reduce()\n",
    "  self &operator+=(T rhsVal)\n",
    "  {\n",
    "    parent::reduce(rhsVal);\n",
    "    return *this;\n",
    "  }\n",
    "\n",
    "  //! enable operator+= for ReduceSum -- alias for reduce()\n",
    "  const self &operator+=(T rhsVal) const\n",
    "  {\n",
    "#ifdef __SYCL_DEVICE_ONLY__\n",
    "    auto i = __spirv::initLocalInvocationId<1, cl::sycl::id<1>>()[0];\n",
    "    auto atm = cl::sycl::ONEAPI::atomic_ref<T, cl::sycl::ONEAPI::memory_order::relaxed, cl::sycl::ONEAPI::memory_scope::device, cl::sycl::access::address_space::global_space>(parent::val.device[i]);\n",
    "    atm.fetch_add(rhsVal);\n",
    "    return *this;\n",
    "#else\n",
    "    parent::reduce(rhsVal);\n",
    "    return *this;\n",
    "#endif\n",
    "  }\n",
    "};\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
