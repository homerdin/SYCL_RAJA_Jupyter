{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAJA Performance Suite - Base_CUDA to Base_SYCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement our `Base_SYCL` variant we ported from the existing `Base_CUDA` variant.  The `Base_CUDA` variant for the `DAXPY` kernel is given.\n",
    "```c\n",
    "namespace rajaperf\n",
    "{\n",
    "namespace basic\n",
    "{\n",
    "\n",
    "  //\n",
    "  // Define thread block size for CUDA execution\n",
    "  //\n",
    "  const size_t block_size = 256;\n",
    "\n",
    "\n",
    "#define DAXPY_DATA_SETUP_CUDA \\\n",
    "  allocAndInitCudaDeviceData(x, m_x, iend); \\\n",
    "  allocAndInitCudaDeviceData(y, m_y, iend);\n",
    "\n",
    "#define DAXPY_DATA_TEARDOWN_CUDA \\\n",
    "  getCudaDeviceData(m_y, y, iend); \\\n",
    "  deallocCudaDeviceData(x); \\\n",
    "  deallocCudaDeviceData(y);\n",
    "\n",
    "__global__ void daxpy(Real_ptr y, Real_ptr x,\n",
    "                      Real_type a,\n",
    "                      Index_type iend)\n",
    "{\n",
    "   Index_type i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "   if (i < iend) {\n",
    "     DAXPY_BODY;\n",
    "   }\n",
    "}\n",
    "\n",
    "\n",
    "void DAXPY::runCudaVariant(VariantID vid)\n",
    "{\n",
    "  const Index_type run_reps = getRunReps();\n",
    "  const Index_type ibegin = 0;\n",
    "  const Index_type iend = getRunSize();\n",
    "\n",
    "  DAXPY_DATA_SETUP;\n",
    "\n",
    "  if ( vid == Base_CUDA ) {\n",
    "\n",
    "    DAXPY_DATA_SETUP_CUDA;\n",
    "\n",
    "    startTimer();\n",
    "    for (RepIndex_type irep = 0; irep < run_reps; ++irep) {\n",
    "\n",
    "      const size_t grid_size = RAJA_DIVIDE_CEILING_INT(iend, block_size);\n",
    "      daxpy<<<grid_size, block_size>>>( y, x, a,\n",
    "                                        iend );\n",
    "\n",
    "    }\n",
    "    stopTimer();\n",
    "\n",
    "    DAXPY_DATA_TEARDOWN_CUDA;\n",
    "\n",
    "  } else if ( vid == RAJA_CUDA ) {\n",
    "\n",
    "    // We will look at this in notebook 3\n",
    "\n",
    "  } else {\n",
    "     std::cout << \"\\n  DAXPY : Unknown Cuda variant id = \" << vid << std::endl;\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "The `DAXPY_DATA_SETUP_CUDA` macro calls `allocAndInitCudaDeviceData` which is used to simplify the memory calls.\n",
    "\n",
    "```c\n",
    "  cudaMalloc( (void**)&dptr,\n",
    "              len * sizeof(typename std::remove_pointer<T>::type) );\n",
    "              \n",
    "  cudaMemcpy( dptr, hptr,\n",
    "              len * sizeof(typename std::remove_pointer<T>::type),\n",
    "              cudaMemcpyHostToDevice );\n",
    "                          \n",
    "```\n",
    "Using the USM features introduced in the SYCL 2020 provisional spec we can implement very similar functionality. We can use the USM API `sycl::malloc_device` to allocate space on the device.  `sycl::malloc_device` returns the pointer to the newly allocated device memory and doesn't pass the device pointer as an argument.  After the size argument we need to pass `sycl::malloc_device` either the device and context or the queue.  In this code we have a public static class member holding our queue named `qu`.\n",
    "\n",
    "```c\n",
    "  dptr = cl::sycl::malloc_device<typename std::remove_pointer<T>::type>(len, qu);\n",
    "```\n",
    "\n",
    "The memcpy call for SYCL is similar but it is a function member of the queue, `qu.memcpy`.  The memcpy call is asynchronous so we will wait on the returned event to ensure the memory is where we need it.\n",
    "\n",
    "```c\n",
    "  auto e = qu.memcpy( dptr, hptr,\n",
    "                      len * sizeof(typename std::remove_pointer<T>::type));\n",
    "  e.wait();\n",
    "```\n",
    "\n",
    "We implement this data setup specific to the `DAXPY` kernel below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/RAJAPerf/src/basic/DAXPY_Setup.hpp\n",
    "x = cl::sycl::malloc_device<Real_type>(iend, qu);\n",
    "y = cl::sycl::malloc_device<Real_type>(iend, qu);\n",
    "auto e = qu.memcpy(x, m_x, iend * sizeof(Real_type));\n",
    "auto e2 = qu.memcpy(y, m_y, iend * sizeof(Real_type));\n",
    "// Wait for memcpys to finish\n",
    "e.wait();\n",
    "e2.wait();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel\n",
    "The CUDA kernel launch is defining a grid size and block size.\n",
    "```c\n",
    "      const size_t grid_size = RAJA_DIVIDE_CEILING_INT(iend, block_size);\n",
    "      daxpy<<<grid_size, block_size>>>( y, x, a,\n",
    "                                        iend );\n",
    "```\n",
    "In SYCL we can use `sycl::nd_range`, which takes in two arguments, the global iteration size and the block size.  We calculate our global size to be a multiple of the block size that fits our iteration space.   The kernel looks very similar expect that we can include it inline with our kernel launch.\n",
    "\n",
    "```c\n",
    "  const size_t global_size = block_size * RAJA_DIVIDE_CEILING_INT(iend, block_size);\n",
    "\n",
    "  qu.submit([&] (cl::sycl::handler& h) {\n",
    "    h.parallel_for<class DAXPY>(cl::sycl::nd_range<1>(global_size, block_size),\n",
    "                                [=] (cl::sycl::nd_item<1> item ) {\n",
    "\n",
    "      Index_type i = item.get_global_id(0);\n",
    "      // We could also calculate the global index\n",
    "      //Index_type i = item.get_group(0) * item.get_local_range(0) + item.get_local_id(0);\n",
    "      if (i < iend) {\n",
    "        DAXPY_BODY\n",
    "      }\n",
    "                                    \n",
    "    });\n",
    "  });\n",
    "```\n",
    "\n",
    "Within the kernel, we access our index through the `sycl::nd_item` object. We  access our global index through `item.get_global_id`, we could also access our global index by calculating directly from our group and local index information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/RAJAPerf/src/basic/DAXPY_Kernel.hpp\n",
    "      const size_t global_size = block_size * RAJA_DIVIDE_CEILING_INT(iend, block_size);\n",
    "\n",
    "      qu.submit([&] (cl::sycl::handler& h) {\n",
    "        h.parallel_for(cl::sycl::nd_range<1>(global_size, block_size),\n",
    "                       [=] (cl::sycl::nd_item<1> item ) {\n",
    "\n",
    "          Index_type i = item.get_global_id(0);\n",
    "          // We could also calculate the global index\n",
    "          // Index_type i = item.get_group(0) * item.get_local_range(0) + item.get_local_id(0);\n",
    "          if (i < iend) {\n",
    "            DAXPY_BODY\n",
    "          }\n",
    "\n",
    "        });\n",
    "      });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Teardown\n",
    "The `DAXPY_DATA_TEARDOWN_CUDA` macro calls `getCudaDeviceData` and `deallocCudaDeviceData` which are wrappers for:\n",
    "```c\n",
    "  cudaMemcpy( hptr, dptr,\n",
    "              len * sizeof(typename std::remove_pointer<T>::type),\n",
    "              cudaMemcpyDeviceToHost );\n",
    "  // and\n",
    "  cudaFree( dptr );\n",
    "              \n",
    "```\n",
    "\n",
    "Using the SYCL 2020 provisional spec the `memcpy` looks the same as when we transfered the data to the device, except that we will switch the desitination and source arguments.  After we wait for the memcpy to finish we will free the device memory with `sycl::free`.\n",
    "\n",
    "```c\n",
    "  auto e = qu.memcpy( dptr, hptr,\n",
    "                      len * sizeof(typename std::remove_pointer<T>::type));\n",
    "  e.wait();\n",
    "\n",
    "  cl::sycl::free(dptr, qu);\n",
    "```\n",
    "\n",
    "We implement this specifically for the `DAXPY` kernel below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/RAJAPerf/src/basic/DAXPY_Teardown.hpp\n",
    "auto e3 = qu.memcpy(m_y, y, iend * sizeof(Real_type));\n",
    "// Wait for memcpy to finish\n",
    "e3.wait();\n",
    "cl::sycl::free(x, qu);\n",
    "cl::sycl::free(y, qu);\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When we put it all together\n",
    "```\n",
    "namespace rajaperf\n",
    "{\n",
    "namespace basic\n",
    "{\n",
    "\n",
    "  //\n",
    "  // Define thread block size for SYCL execution\n",
    "  //\n",
    "  const size_t block_size = 256; // We could query our device for this\n",
    "\n",
    "#define DAXPY_DATA_SETUP_SYCL                            \\\n",
    "  x = cl::sycl::malloc_device<Real_type>(iend, qu);      \\\n",
    "  y = cl::sycl::malloc_device<Real_type>(iend, qu);      \\\n",
    "  auto e = qu.memcpy(x, m_x, iend * sizeof(Real_type));  \\\n",
    "  auto e2 = qu.memcpy(y, m_y, iend * sizeof(Real_type)); \\\n",
    "  e.wait(); \\\n",
    "  e2.wait();\n",
    "\n",
    "\n",
    "#define DAXPY_DATA_TEARDOWN_SYCL                        \\\n",
    "  auto e = qu.memcpy(m_x, x, iend * sizeof(Real_type)); \\\n",
    "  e.wait();                                             \\\n",
    "  cl::sycl::free(x, qu);                                \\\n",
    "  cl::sycl::free(y, qu);\n",
    "                \n",
    "\n",
    "void DAXPY::runSyclVariant(VariantID vid)\n",
    "{\n",
    "  const Index_type run_reps = getRunReps();\n",
    "  const Index_type ibegin = 0;\n",
    "  const Index_type iend = getRunSize();\n",
    "\n",
    "  DAXPY_DATA_SETUP; // This sets up our host data. m_x, m_y.\n",
    "\n",
    "  if ( vid == Base_SYCL ) {\n",
    "    { // Create a scope for our buffers\n",
    "\n",
    "      DAXPY_DATA_SETUP_SYCL;\n",
    "\n",
    "      startTimer();\n",
    "      for (RepIndex_type irep = 0; irep < run_reps; ++irep) {\n",
    "\n",
    "        const size_t global_size = block_size \n",
    "                                   * RAJA_DIVIDE_CEILING_INT(iend, block_size);\n",
    "\n",
    "        qu.submit([&] (cl::sycl::handler& h) {\n",
    "          h.parallel_for(cl::sycl::nd_range<1>{global_size, block_size},\n",
    "                         [=] (cl::sycl::nd_item<1> item ) {\n",
    "\n",
    "            Index_type i = item.get_global_id(0);\n",
    "            if (i < iend) {\n",
    "              DAXPY_BODY\n",
    "            }\n",
    "\n",
    "          });\n",
    "        });\n",
    "      }\n",
    "      qu.wait(); // Wait for computation to finish before stopping timer\n",
    "      stopTimer();\n",
    "     \n",
    "    } // End of buffer scope\n",
    "\n",
    "    DAXPY_DATA_TEARDOWN_SYCL;\n",
    " \n",
    "  } else if ( vid == RAJA_SYCL ) {\n",
    "\n",
    "  // We will do this later\n",
    "\n",
    "  } else {\n",
    "     std::cout << \"\\n  DAXPY : Unknown Sycl variant id = \" << vid << std::endl;\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "} // end namespace basic\n",
    "} // end namespace rajaperf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets rebuild the performance suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: Build: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!qsub build_RAJAPerf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After it finishes building, lets run the performance suite with our `Base_SYCL DAXPY` kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -l nodes=1:gpu:ppn=2 run_RAJAPerf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the run finishes, lets check the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat output/RAJAPerf-timing.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
